
/*
 * @PACKAGE_NAME@ @PACKAGE_VERSION@ Copyright 2001 Neo Natura
 *
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *    The Encoded Stream Protocol (ESTP).
 *
 *  This file is part of the Share Library.
 *  (https://github.com/neonatura/share)
 *        
 *  The Share Library is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation, either version 3 of the License, or
 *  (at your option) any later version. 
 *
 *  The Share Library is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with The Share Library.  If not, see <http://www.gnu.org/licenses/>.
 */

 
/* RFC1122 Status:
   4.1.3.1 (Ports):
     SHOULD send ICMP_PORT_UNREACHABLE in response to datagrams to an un-listened port. (YES)
   4.1.3.2 (IP Options)
     MUST pass IP options from IP -> application (OK)
     MUST allow application to specify IP options (OK)
   4.1.3.3 (ICMP Messages)
     MUST pass ICMP error messages to application. (OK)

	 ( ESTP CHECKSUMS )
     MUST provide atleast one facility for checksumming (OK)
     MUST allow application to control checksumming behavior (OK)
     MUST default to checksumming ON (OK)
		 MAY attempt retransmission of packets with bad csums. (YES)

	 ( ?? )
     MUST disregard packets with packet parameters errors (OK)
		 MAY disregard packets with errors concerning access violations (YES)
		 MAY disregard packets with errors concerning retransmission (YES)
		 MUST disregard packets with unknown errors (OK)

   4.1.3.5 (esp Multihoming)
     MUST allow application to specify source address (OK)
     SHOULD be able to communicate the chosen src addr up to application
       when application doesn't choose (DOES - use recvmsg cmsgs)

   4.1.3.6 (Invalid Addresses)
     MUST discard invalid source addresses (OK -- done in the new routing code)
     MUST only send data with one of our addresses (OK)
*/

#include <asm/system.h>
#include <asm/uaccess.h>
#include <linux/types.h>
#include <linux/fcntl.h>
#include <linux/module.h>
#include <linux/version.h>
#include <linux/kernel.h>
#include <linux/socket.h>
#include <linux/sockios.h>
#include <linux/in.h>
#include <linux/errno.h>
#include <linux/timer.h>
#include <linux/mm.h>
#include <linux/inet.h>
#include <linux/netdevice.h>
#include <net/snmp.h>
#include <net/ip.h>
#include <net/protocol.h>
#include <linux/skbuff.h>
#include <net/sock.h>
#include <net/inet_hashtables.h>
#include <net/icmp.h>
#include <net/route.h>
#include <linux/init.h>
#include <linux/sched.h>
#include <net/checksum.h>
#include <linux/module.h>
#include <linux/ip.h>
#include <net/inet_sock.h>
#ifdef CONFIG_KMOD
#include <linux/kmod.h>
#endif
#include "shesp.h"

#undef NETDEBUG
#define NETDEBUG(_a)

#undef CONFIG_IPV6
#undef CONFIG_IPV6_MODULE

struct inet_hashinfo shesp_hashinfo;
struct inet_timewait_death_row shesp_death_row;


struct esp_mib esp4_statistics; 
struct esp_mib esp6_statistics; 


#if 0
int esp_v4_verify_bind(struct sock *sk, unsigned short snum)
{
	struct sock *sk2;
	int reuse1 = sk->sk_reuse;
	int retval = 0;

	sk_for_each (sk, 
	write_lock_bh(&es_hash_lock);
	for(sk2 = es_hash[snum & (ESTP_HTABLE_SIZE - 1)]; sk2 != NULL; sk2 = sk_next(sk2)) {
		if((sk2->num == snum) && (sk2 != sk)) {
			unsigned char state = sk2->state;
			int reuse2 = sk2->reuse;

			/* Two sockets can be bound to the same port if they're
			 * bound to different interfaces.
			 */
			if(sk2->bound_dev_if != sk->bound_dev_if)
				continue;

			/* Multiple protocol sets may exist in the same hash table. */
			if (sk2->sk_protocol != sk->sk_protocol)
				continue;

			if(!sk2->rcv_saddr || !sk->rcv_saddr) {
				if( (!reuse2)		||
						(!reuse1)			||
						(state == ESTP_LISTEN)) {
					retval = 1;
					break;
				}
			} else if(sk2->rcv_saddr == sk->rcv_saddr) {
				if((!reuse1)			||
				   (!reuse2)			||
				   (state == ESTP_LISTEN)) {
					retval = 1;
					break;
				}
			}
		}
	}
	write_unlock_bh(&es_hash_lock);

	NETDEBUG(ESTPDEBUG(printk(KERN_DEBUG "NET4: ESTP: esp_v4_verify_bind: port %d ret'd %d.\n", (int)snum, retval)));

	return (retval);
}
#endif

#if 0
struct sock *esp_v4_rcv_lookup(u32 saddr, u16 sport, u32 daddr, u16 dport, int dif, __u32 stream_id)
{
	struct sock *sk, *result = NULL;
  struct hlist_node *node;
  unsigned short hnum = ntohs(dport);
  int badness = -1;

	sk_for_each(sk, node, &es_hash[hnum & (ESTP_HTABLE_SIZE - 1)]) {

  sk_for_each(sk, node, &es_hash[hnum & (ESTP_HTABLE_SIZE - 1)]) {
    struct inet_sock *inet = inet_sk(sk);
		struct es_sock *af2 = es_sk(sk);

		if (af2->r_id != stream_id)
			continue;

		if (sk->sk_protocol != IPPROTO_ESTP)
			continue;

		if ((inet->num == hnum) && 
				!(sock_flag(sk, SOCK_DEAD) && (sk->sk_state == ESTP_CLOSE)) &&
				!ipv6_only_sock(sk)) {
			int score = (sk->sk_family == PF_INET ? 1 : 0);
			if (inet->rcv_saddr) {
				if (inet->rcv_saddr != daddr)
					continue;
				score+=2;
			}
			if (inet->daddr) {
				if (inet->daddr != saddr)
					continue;
				score+=2;
			}
			if (inet->dport) {
				if (inet->dport != sport)
					continue;
				score+=2;
			}
			if (sk->sk_bound_dev_if) {
				if (sk->sk_bound_dev_if != dif)
					continue;
				score+=2;
			}
			if(score == 9) {
				result = sk;
				break;
			} else if(score > badness) {
				result = sk;
				badness = score;
			}
		}
	}
	return result;
}
#endif

static void esp_v4_sync_mss(struct sock *sk, u32 pmtu)
{
  struct inet_connection_sock *icsk = inet_csk(sk);

  icsk->icsk_pmtu_cookie = pmtu;
}

/**
 * This routine does path mtu discovery as defined in RFC1191.
 */
static void do_pmtu_discovery(struct sock *sk, struct iphdr *iph, u32 mtu)
{
  struct dst_entry *dst;
  struct inet_sock *inet = inet_sk(sk);

  /* We are not interested in ESTP_LISTEN and open_requests (SYN-ACKs
   * send out by Linux are always <576bytes so they should go through
   * unfragmented).
   */
  if (sk->sk_state == ESTP_LISTEN)
    return;

  /* We don't check in the destentry if pmtu discovery is forbidden
   * on this route. We just assume that no packet_to_big packets
   * are send back when pmtu discovery is not active.
   * There is a small race when the user changes this flag in the
   * route, but I think that's acceptable.
   */
  if ((dst = __sk_dst_check(sk, 0)) == NULL)
    return;

  dst->ops->update_pmtu(dst, mtu);

  /* Something is about to be wrong... Remember soft error
   * for the case, if this connection will not able to recover.
   */
  if (mtu < dst_mtu(dst) && ip_dont_fragment(sk, dst))
    sk->sk_err_soft = EMSGSIZE;

  mtu = dst_mtu(dst);

  if (inet->pmtudisc != IP_PMTUDISC_DONT &&
      inet_csk(sk)->icsk_pmtu_cookie > mtu) {
    inet_csk(sk)->icsk_pmtu_cookie = mtu;

    /* Resend the ESTP packet because it's
     * clear that the old packet has been
     * dropped. This is the new "fast" path mtu
     * discovery.
     */
    //esp_simple_retransmit(sk);
  } /* else let the usual retransmit timer handle it */
}


/*
 * This routine is called by the ICMP module when it gets some
 * sort of error condition.  If err < 0 then the socket should
 * be closed and the error returned to the user.  If err > 0
 * it's just the icmp type << 8 | icmp code.  
 * Header points to the ip header of the error packet. We move
 * on past this. Then (as it used to claim before adjustment)
 * header points to the first 8 bytes of the esp header.  
 */
void shesp_v4_err(struct sk_buff *skb, u32 info)
{
  struct iphdr *iph = (struct iphdr *)skb->data;
  struct esphdr *eh = (struct esphdr *)(skb->data + (iph->ihl << 2));
  struct net *net = dev_net(skb->dev);
  const int type = icmp_hdr(skb)->type;
  const int code = icmp_hdr(skb)->code;
	struct sock *sk;
	int err;

	err = 0;

	NETDEBUG(ESTPDEBUG(printk (KERN_DEBUG "NET4: ESTP: esp_v4_err: type=%d, code=%d.\n", type, code)));

	if (skb->len < (iph->ihl<<2) + sizeof(struct esphdr)) {
    ICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);
		return;
	}

  sk = inet_lookup(dev_net(skb->dev), &shesp_hashinfo, iph->daddr, eh->dport, iph->saddr, eh->sport, inet_iif(skb));
	//sk = esp_v4_rcv_lookup(iph->daddr, eh->dport, iph->saddr, eh->sport, skb->dev ? skb->dev->ifindex : 0, eh->stream_id);
	if (sk == NULL) {
    ICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);
		return;	/* No socket for error */
	}

	bh_lock_sock(sk);

	/* If too many ICMPs get dropped on busy
	 * servers this needs to be solved differently.
	 */
	if (sock_owned_by_user(sk))
    NET_INC_STATS_BH(net, LINUX_MIB_LOCKDROPPEDICMPS);
	
	if (sk->sk_state == ESTP_LISTEN) {
		/* socket in error was a listen socket. */
		NET_INC_STATS_BH(net, ICMP_MIB_INERRORS);
		goto out; /* invalid socket for error. */
	}

	if (sk->sk_state == ESTP_CLOSE) {
		/* socket in error has already been closed. */
		NET_INC_STATS_BH(net, ICMP_MIB_INERRORS);
		goto out; /* invalid socket for error */
	}

	switch (type) {
		case ICMP_SOURCE_QUENCH: 
			/* icmp request discarded. */
			goto out;
		case ICMP_PARAMETERPROB:
			err = EPROTO;
			break;
		case ICMP_DEST_UNREACH:
			if (code > NR_ICMP_UNREACH)
				goto out;

			if (code == ICMP_FRAG_NEEDED) { /* PMTU Discovery (RFC #1191) */
				if (!sock_owned_by_user(sk))
					do_pmtu_discovery(sk, iph, info);
				goto out;
			}

			err = icmp_err_convert[code].errno;
			break;
		case ICMP_TIME_EXCEEDED:
			err = EHOSTUNREACH;
			break;
		case ICMP_INFO_REQUEST:
			/* obsolete ICMP_INFO_REQUESTS discarded. */
			goto out;
		case ICMP_INFO_REPLY:
			/* obsolete ICMP_INFO_REPLY discarded. */
			goto out;
		case ICMP_ADDRESS:
			/* obsolete ICMP_ADDRESS discarded. */
			goto out;
		case ICMP_ADDRESSREPLY:
			/* obsolete ICMP_ADDRESSREPLY discarded. */
			goto out;
	}

	if (!sock_owned_by_user(sk) && inet_sk(sk)->recverr) {
		sk->sk_err = err;
		sk->sk_error_report(sk);
	} else  { /* only an error on timeout */
		sk->sk_err_soft = err;
	}

out:
	bh_unlock_sock(sk);
	sock_put(sk);
}

static void __esp_v4_xmit_timer(struct sock *sk)
{
  struct shesp_sock *af = shesp_sk(sk);
	struct sk_buff *skb = NULL;
	struct esphdr *skb_eh;
	struct esp_queue_s *q = NULL;
	struct iphdr *iph;
	struct esphdr eh;
  u_char *skb_raw;
	ushort pktdata_len, pktdata_del;
	u_char *pktdata = NULL;
	int next_ss, asl, length;
	int pm, flags;
	int msg_flags;
	int xmit_cnt;
	int nextq_size;
	int hh_len; 
	int df, err;

	if (!sk->sk_dst_cache) {
		/* 
		 * [!sk->sk_dst_cache] conditions:
		 * 1. The socket has never initialized a route destination.
		 * 2. The socket is temporarily without a route destination.
		 *
		 * BUG: listen socket should maybe remove the queue timer,
		 *      as it does not need xmits or probes performed. 
		 */
NETDEBUG(ESTPDEBUG(printk(KERN_DEBUG "NET4: ESTP: DEBUG: esp_v4_xmit_timer: !sk->sk_dst_cache\n")));
		return;
	}

#if 0
#ifdef CONFIG_ESTP_DEVICE_BALANCE
	if (EOPT_ISSET(sk, EOPT_DEVICE_BALANCE) && !sk->bound_dev_if) {
		struct rtable *rt = (struct rtable *)sk->sk_dst_cache;
    struct inet_sock *inet = inet_sk(sk);
		struct rtable *new_rt;
		struct device *dev;

		dev = sk->sk_dst_cache->dev->next;
		for (; dev != sk->sk_dst_cache->dev; dev = dev->next) {
			if (!dev)
				dev = dev_base;
			//err = ip_route_connect(&new_rt, rt->rt_dst, rt->rt_src, sk->protinfo.af_inet.tos | sk->localroute, dev->ifindex); 
      err = ip_route_connect(&new_rt, rt->rt_dst, rt->rt_src, RT_CONN_FLAGS(sk), dev->ifindex, IPPROTO_ESTP, inet->sport, inet->dport, sk, 0);
			if (!err) {
				if (new_rt->rt_flags & RTCF_BROADCAST && !sk->broadcast) {
					ip_rt_put(new_rt);
					continue;
				}
				dst_release(xchg(&sk->sk_dst_cache, &new_rt->u.dst));
			}
		}

	}
#endif
#endif

	hh_len = (sk->sk_dst_cache->dev->hard_header_len + 15)&~15;
	for (xmit_cnt = 0; xmit_cnt < af->pck_per_burst; xmit_cnt++) {

		/* obtain the next esp queue item for transmission. */
		if (!(q = esp_pull_queue(sk, &af->s_queue)))
			break;

		err = 0;
		pm = q->pm;
		flags = q->esp_flags;
		msg_flags = q->msg_flags;

		if (flags & EOPT_RETRAN) {
			int pkt_seq, pkt_asl;

			memcpy(&eh, &q->eh, sizeof(struct esphdr));
			pkt_seq = ntohs(eh.seq);
			pkt_asl = ntohs(eh.asl);
			next_ss = pkt_asl<pkt_seq ? pkt_asl+65536 : pkt_asl;
			asl = next_ss - pkt_seq;
		} else { /* not a RETRAN packet. */
			if (pm >= ESTP_PACKET_MODES) {
				NETDEBUG(ESTPDEBUG(printk(KERN_DEBUG "NET4: ESTP: %d.%d.%d.%d:%d - %d.%d.%d.%d:%d: __esp_v4_xmit_timer: pm >= ESTP_PACKET_MODES (%d)\n", ESTP_SKADDR(sk), (int)pm)));
				goto error;
			}
			if (!af->s_seq[pm] && !af->ns_seq[pm]) {
				asl = 0; 
			} else {
				next_ss = 
					af->ns_seq[pm]<af->s_seq[pm] ? af->ns_seq[pm]+65536 : af->ns_seq[pm];
				asl = next_ss - af->s_seq[pm]; /* assumed data length */ 
			} 
		}

		if (pm == ESTP_DATA) {
			err = esp_encode_packet(sk, &q, asl ? asl : af->msl, 
					&pktdata, &pktdata_len, &pktdata_del);
			if (err) {
				NETDEBUG(ESTPDEBUG(printk(KERN_DEBUG "NET4: ESTP: esp_encode_packet: error %d.\n", err)));
				goto error;
			}
		} else {
			pktdata_len = q->data_len;
			pktdata = (u_char *)kmalloc(pktdata_len, GFP_ATOMIC);
			if (!pktdata) {
				err = -ENOMEM;
				goto error;
			}
			memcpy(pktdata, q->data_raw, pktdata_len);
			pktdata_del = 0;
		}

		if (!asl)
			asl = pktdata_len;

		if (!(flags & EOPT_RETRAN)) {
			if (pm != ESTP_HANDSHAKE) {
				eh.stream_id = af->s_id; /* nbo */
			} else {
				eh.stream_id = ESTP_DEF_STREAM_ID; 
			}
			eh.sport      = inet_sk(sk)->sport;
			eh.dport      = inet_sk(sk)->dport; 
			eh.pm         = pm;            /* packet mode. */
			eh.ver        = ESTP_VERSION;
			eh.opt        = (flags & 0xFF);                /* "opt" is 1 byte. */ 
			eh.del        = htons(pktdata_del);
			eh.len        = htons(sizeof(esphdr)+pktdata_len);
			eh.seq        = htons(af->s_seq[pm]);
			af->s_seq[pm] = (__u16)(((int)af->s_seq[pm] + asl) % 65536);
			eh.asl        = htons(af->s_seq[pm]);
			if (pm == ESTP_DATA) {
				if (af->s_queue && af->s_queue->pm == ESTP_DATA &&
						(nextq_size = min(af->msl, esp_packet_size(sk, af->s_queue))) > 0) {
					af->ns_seq[ESTP_DATA] = (__u16)((af->s_seq[pm] + nextq_size) % 65536); 
				} else {
					af->ns_seq[ESTP_DATA] = (__u16)((af->s_seq[pm] + af->sl) % 65536);
				}
			} else if (pm == ESTP_PROBE) {
				af->ns_seq[ESTP_PROBE] = (__u16)(((int)af->s_seq[ESTP_PROBE] + af->probe.len) % 65536);
			} else {
				af->ns_seq[pm] = (__u16)(((int)af->s_seq[pm] + pktdata_len) % 65536);
			}
			eh.nsl = (__u16)htons((unsigned short int)af->ns_seq[pm]);
		}

		/* Fast path for ip unfragmented frames without options. */ 
		length = sizeof(struct iphdr) + sizeof(esphdr) + asl;
		skb = sock_alloc_send_skb(sk, length+hh_len+15, 
				(msg_flags & MSG_DONTWAIT), &err);
		if (!skb)
			goto error;
		skb_reserve(skb, hh_len);

		skb->priority = sk->sk_priority;
    sk_dst_set(sk, dst_clone(sk->sk_dst_cache));

		df = 0;
#ifdef ESTP_USE_PMTU_DISC
		/* The "ip_dont_fragment()" function handles ipv4 pmtu discovery. */
		if (ip_dont_fragment(sk, sk->sk_dst_cache))
			df = htons(IP_DF);
#else
		/* 
		 * The IPv4 pmtu discovery process is only performed on 
		 * the ESTP Probe Packet. 
		 */
		if (pm == ESTP_PROBE) {
			df = htons(IP_DF);
		}
#endif

/* bb 112914 */
		//skb->nh.iph = iph = (struct iphdr *)skb_put(skb, length);
		skb_raw = (u_char)skb_put(skb, length - sizeof(struct iphdr));
		skb_eh = (struct esphdr *)skb_raw;


/* bb 112914 */
		//dev_lock_list();
		
#if 0
		memset(iph, 0, length);
		iph->saddr      = sk->saddr;
		iph->daddr      = sk->daddr;
		iph->version    = 4;
		iph->ihl        = 5;
		iph->tos        = inet_sk(sk)->tos;
		iph->tot_len    = htons(length);
		iph->id         = htons(ip_id_count++);
		iph->frag_off   = df;
    iph->ttl = inet_sk(sk)->uc_ttl;
		iph->protocol   = sk->sk_protocol;

		/* IPv4 checksum. */
		iph->check = 0;
		iph->check = ip_fast_csum((unsigned char *)iph, iph->ihl);

		eh_p = (struct esphdr *)((char *)iph + iph->ihl*4);
#endif

		/* copy ESTP header to socket buffer. */
		memcpy(skb_eh, &eh, sizeof(esphdr));

		/* copy raw packet data to socket buffer. */
		//memcpy((char *)iph + iph->ihl*4 + sizeof(esphdr), pktdata, pktdata_len);
		memcpy((char *)skb_eh + sizeof(esphdr), pktdata, pktdata_len);

		/* ESTP Checksum. */
		skb_eh->check = ESTP_DEF_CHECK;
		if (pm != ESTP_HANDSHAKE) {
			skb_eh->check = esp_check(sk, af->s_id, skb_raw, sizeof(esphdr)+pktdata_len);
		} 

/* bb 112914 */
		//dev_unlock_list();

		memcpy(&eh, skb_eh, sizeof(esphdr)); /* for backlog below. */

#if 0
#ifdef CONFIG_FIREWALL
		{ 
			int fw_status;

			fw_status = call_out_firewall(PF_INET, 
					sk->sk_dst_cache->dev, iph, NULL, &skb);
			switch (fw_status) {
				case FW_REJECT:
					icmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);
				case FW_BLOCK:
					err = -EPERM;
					goto error;
				case FW_QUEUE:
					/* continue without error. */
					kfree_skb(skb);
					goto next;
			}
		}
#endif
#endif

		if (af->rem_host) /* CONF */
			af->rem_host->sw = af->rem_host->sw - asl;

NETDEBUG(ESTPDEBUG(printk(KERN_DEBUG "INET: ESTP: DEBUG: xmit %d size packet of mode %d.\n", length, pm)));

		/* transmit socket buffer */
    err = ip_build_and_send_pkt(skb, sk, inet_sk(sk)->saddr, inet_sk(sk)->daddr, shesp_ip_opt(sk));
/*
		sk->sk_dst_cache->output(skb);
		esp_statistics.EspOutPackets++;
		if (pm == ESTP_DATA)
			esp_statistics.EspOutDataPackets++;
*/

next:
		if (!(flags & EOPT_RETRAN)) {
			/* backlog packet */ 
			err = esp_backlog_packet(sk, &pktdata, pktdata_len, &eh, flags);
			if (err)
				goto error;
		} else {
			/* Free local copy of raw trasmit data. */
			kfree(xchg(&pktdata, NULL));
		}

		kfree_queue(&q);
	}

	if (xmit_cnt == af->pck_per_burst && /* max packets per burst was sent. */
			!af->pck_loss_rate) {            /* no packets were lost "lately". */
		/* increment the number of packets sent per 'burst'. */
		af->pck_per_burst += ESTP_INCR_BURST_RATE;
	}

	return;

error:
	NETDEBUG(printk(KERN_DEBUG "NET4: ESTP: __esp_queue_timer: %d.%d.%d.%d:%d - %d.%d.%d.%d:%d: discarded an IP packet (err=%d).\n", ESTP_SKADDR(sk), err));
	sk->sk_err = -err;
	if (pktdata)
		kfree(pktdata);
	if (skb)
		kfree_skb(skb);
	kfree_queue(&q);
	//ip_statistics.IpOutDiscards++;
}

#define TENTH_HZ (HZ/10)
static void __esp_v4_queue_timer(unsigned long data)
{
  struct sock *sk = (struct sock *)data;
  struct inet_connection_sock *icsk = inet_csk(sk);

  bh_lock_sock(sk);
  if (sock_owned_by_user(sk)) 
    goto done;

  if (time_after(icsk->icsk_timeout, jiffies))
    goto done_reclaim;

	__esp_v4_xmit_timer(sk);

done_reclaim:
  sk_mem_reclaim(sk);
done:
  sk_reset_timer(sk, &icsk->icsk_retransmit_timer, jiffies + TENTH_HZ);
  bh_unlock_sock(sk);
}
#undef TENTH_HZ

/**
 * Transmits pending outgoing data.
 */
static void __esp_v4_default_timer(unsigned long data)
{
  struct sock *sk = (struct sock *)data;
  struct shesp_sock *af = shesp_sk(sk);
	unsigned long int now = jiffies;
  struct inet_connection_sock *icsk = inet_csk(sk);

  if (time_after(icsk->icsk_ack.timeout, jiffies)) {
    sk_reset_timer(sk, &icsk->icsk_delack_timer, icsk->icsk_ack.timeout);
    return;
  }


  sk_reset_timer(sk, &icsk->icsk_delack_timer, icsk->icsk_ack.timeout);

#if 0
	if (sock_flag(sk, SOCK_DEAD) || 
			sock_flag(sk, SOCK_DONE) || 
      sk->sk_state != ESTP_ESTABLISHED)
		goto done;

  /* eh.. */

done:
  sk_reset_timer(sk, &icsk->icsk_retransmit_timer, jiffies + HZ);
	if (now + HZ > jiffies) {
		mod_timer(&af->timer, now + HZ);
	} else {
		mod_timer(&af->timer, jiffies + HZ);
	}
#endif
}

static void __esp_v4_probe_timer(unsigned long data)
{
  struct sock *sk = (struct sock *) data;
  struct inet_connection_sock *icsk = inet_csk(sk);
  struct shesp_sock *af = shesp_sk(sk);
  __u32 elapsed;

  bh_lock_sock(sk);

  /* Only process if socket is not in use. */
  if (sock_owned_by_user(sk))
    goto done; /* all done. */

	if (sock_flag(sk, SOCK_DEAD) || 
			sock_flag(sk, SOCK_DONE) || 
      sk->sk_state != ESTP_ESTABLISHED) {
    /* kill timer */
    bh_unlock_sock(sk);
    return;
  }

	/* Probe Timer */
  esp_probe_sendlen(sk);
  af->probe.timeout = af->probe.tinterval;

//  sk_mem_reclaim(sk);

done:
  inet_csk_reset_keepalive_timer(sk, af->probe.tinterval);
  bh_unlock_sock(sk);
// sock_put(sk);
}

static inline int esp_v4_ioctl(struct sock *sk, int cmd, unsigned long arg)
{
	return (esp_ioctl(sk, cmd, arg));
}

static int shesp_v4_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg, size_t len, int nonblock, int flags, int *addr_len)
{
	int err;

	if (addr_len) {
		*addr_len = sizeof(struct sockaddr_in);
	}

	if (flags & MSG_ERRQUEUE) {
		return (ip_recv_error(sk, msg, len));
	}

	err = shesp_recvmsg(iocb, sk, msg, len, nonblock, flags, addr_len);
	if (err)
		return (err);

	return (0);
}

static inline int shesp_v4_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size)
{
	return (shesp_sendmsg(iocb, sock, msg, size));
}

/*
 * todo: may need to est socket to ESTP_CLOSE upon failure.
 */
int shesp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)
{
  struct inet_sock *inet = inet_sk(sk);
  struct sockaddr_in *usin = (struct sockaddr_in *)uaddr;
  struct rtable *rt;
  __be32 daddr, nexthop;
  int tmp;
  int err;

	if (addr_len < sizeof(struct sockaddr_in))
		return -EINVAL;

	if (usin->sin_family != AF_INET)
		return -EAFNOSUPPORT;

  nexthop = daddr = usin->sin_addr.s_addr;
  if (shesp_ip_opt(sk) && shesp_ip_opt(sk)->srr) {
    if (!daddr)
      return -EINVAL;
    nexthop = shesp_ip_opt(sk)->faddr;
  }

  err = ip_route_connect(&rt, nexthop, inet->saddr,
      RT_CONN_FLAGS(sk), sk->sk_bound_dev_if,
      IPPROTO_ESTP, inet->sport, usin->sin_port, sk, 0);
  if (err < 0) {
    if (err == -ENETUNREACH)
      IP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);
    return err;
  }

  if (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {
    ip_rt_put(rt);
    return -ENETUNREACH;
  }

  if (!shesp_ip_opt(sk) || !shesp_ip_opt(sk)->srr)
    daddr = rt->rt_dst; 

	if (!inet->saddr)
		inet->saddr = rt->rt_src;		/* Update source address */
	inet->rcv_saddr = inet->saddr;

	inet->daddr = daddr;
	inet->dport = usin->sin_port;

  inet_csk(sk)->icsk_ext_hdr_len = 0;
  if (shesp_ip_opt(sk))
    inet_csk(sk)->icsk_ext_hdr_len = shesp_ip_opt(sk)->optlen;

  err = inet_hash_connect(&shesp_death_row, sk);
  if (err)
    goto failure;

	err = ip_route_newports(&rt, IPPROTO_ESTP, inet->sport, inet->dport, sk);
	if (err)
    goto failure;

	/* commit destination to socket. */
	sk_setup_caps(sk, &rt->u.dst);

	esp_attach_remote_host(sk);
	esp_set_state(sk, ESTP_CONN_WAIT);

	return (0);

failure:
  esp_set_state(sk, ESTP_CLOSE);
  ip_rt_put(rt);
  sk->sk_route_caps = 0;
  inet->dport = 0;
  return err;
}

#ifdef CONFIG_IP_TRANSPARENT_PROXY
/*
 *	Check whether a received esp packet might be for one of our
 *	sockets.
 */

int esp_chkaddr(struct sk_buff *skb)
{
	struct iphdr *iph = skb->nh.iph;
	struct esphdr *eh = (struct esphdr *)(skb->nh.raw + iph->ihl*4);
  struct net *net = dev_net(skb->dev);
	struct sock *sk;

  sk = inet_lookup(net, &shesp_hashinfo, iph->daddr, eh->dport, iph->saddr, eh->sport, inet_iif(skb));
//	sk = esp_v4_rcv_lookup(iph->saddr, eh->sport, iph->daddr, eh->dport, skb->dev ? skb->dev->ifindex : 0, eh->stream_id);
	if (!sk)
		return 0;

	/* 0 means accept all LOCAL addresses here, not all the world... */
	if (sk->rcv_saddr == 0)
		return 0;

	return 1;
}
#endif

int shesp_v4_rcv(struct sk_buff *skb, unsigned short len)
{
  struct net *net = dev_net(skb->dev);
  struct esphdr *eh;
//  const struct iphdr *iph;
  struct shesp_sock *af;
	struct sock *sk = NULL;
	int pm, seq, asl, nsl;
	u_short elen;
	ushort csum;
	int err = 0;

  if (skb->pkt_type != PACKET_HOST)
    goto done;

  if (!pskb_may_pull(skb, sizeof(struct esphdr))) {
    err = -EBADMSG;
    goto done;
  }

//  iph = ip_hdr(skb);
  eh = shesp_hdr(skb);
	//__skb_pull(skb, skb->h.raw - skb->data);
//	if (len < sizeof(struct esphdr)) { goto done; }

	elen = ntohs(eh->len);
	pm   = eh->pm;
	seq  = ntohs(eh->seq);
	asl  = ntohs(eh->asl);
	nsl  = ntohs(eh->nsl);
	csum = eh->check; /* nbo */

  if (elen < sizeof(struct esphdr))
    goto done;

  if (!pskb_may_pull(skb, elen - sizeof(struct esphdr)))
    goto done;

  if (!skb_csum_unnecessary(skb)) {
    if (pm != ESTP_HANDSHAKE) {
      eh->check = ESTP_DEF_CHECK;
      skb->csum = esp_check(sk, af->r_id, (u_char *)eh, elen);
    } else {
      skb->csum = ESTP_DEF_CHECK;
    }
    if (skb->csum != csum) {
      err = ESE_INVALCRC;
      esp_error(sk, skb, E_CHECKSUM, err);
      goto done;
    } 
  }

  sk = __inet_lookup_skb(&shesp_hashinfo, skb, eh->sport, eh->dport);
	if (!sk) {
		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);
		err = -EHOSTUNREACH;
		goto done;
	}

  if (sk_filter(sk, skb))
    goto release;

  af = shesp_sk(sk);
	if (elen < sizeof(esphdr) || elen > ESTP_MAX_DATA_SIZE) {
		err = ESE_INVALSIZEDATA;
		esp_error(sk, skb, E_LENGTH, err);
		goto done;
	} 

	/* Validate the packet and the esp length. */
	if (elen > len) {
		err = ESE_TRUNCDATA;
		esp_error(sk, skb, E_LENGTH, err);
		goto done;
	}

  bh_lock_sock_nested(sk);
  err = 0;
  if (!sock_owned_by_user(sk)) {
    if (sk->sk_state != ESTP_LISTEN) {
      /* process a incoming IPv4/IPv6 packet and free the skb. */
      err = esp_rcv(sk, &skb, len, pm);
    } else {
      err = esp_listen_rcv(sk, &skb, pm);
    }
  } else {
    sk_add_backlog(sk, skb);
  }
  bh_unlock_sock(sk);

release:
  sock_put(sk);

done:
  if (sk && err) {
    ESTP_STATS(sk)->EspInErrors++;
  }
  if (skb)
    kfree_skb(skb);
	return (0);
}

static inline void shesp_v4_close(struct sock *sk, long timeout)
{
	return (esp_close(sk, timeout));
}

/*
 * Timers for transmit, delayed acks and probes.
 */
static void esp_v4_init_timers(struct sock *sk)
{

  /* bb 112914 */
  inet_csk_init_xmit_timers(sk, &__esp_v4_queue_timer,
      &__esp_v4_default_timer, &__esp_v4_probe_timer);



#if 0
  struct shesp_sock *af = shesp_sk(sk);

	/* ipv4 timer */
  setup_timer(&sk->sk_timer, net_timer, (unsigned long)sk);

	/* esp queue timer */
	init_timer(&af->queue_timer);
	af->queue_timer.expires   = jiffies + HZ/10;
	af->queue_timer.function  = (void (*)(unsigned long)) __esp_v4_queue_timer;
	af->queue_timer.data      = (unsigned long) sk;
	add_timer(&af->queue_timer);

	/* esp default timer */
	init_timer(&af->timer);
	af->timer.expires        = jiffies + HZ;
	af->timer.function       = (void (*)(unsigned long)) __esp_v4_default_timer;
	af->timer.data           = (unsigned long) sk;
	add_timer(&af->timer);

	/* Probe Timer */
	af->probe.timeout        =
	af->probe.tinterval      = 0;
#endif
}

static int esp_v4_init_sock(struct sock *sk)
{
  struct shesp_sock *af = shesp_sk(sk);
	int err;
	
	esp_v4_init_timers(sk);

	err = esp_init_sock(sk);
	if (err)
		return (err);

  inet_sk(sk)->uc_ttl = MAXTTL;

	return (0);
}

static inline int esp_v4_destroy_sock(struct sock *sk)
{
	return (esp_destroy_sock(sk));
}

static int esp_v4_setsockopt(struct sock *sk, int level, int optname, char *optval, int optlen)
{
  struct shesp_sock *af = shesp_sk(sk);
	unsigned long int val=0;
	struct device *dev;
	struct rtable *rt = (struct rtable *)sk->sk_dst_cache, *newrt;
  struct net *net = sock_net(sk);
	int err;
	int i;

	if (level != SOL_ESTP)
		return (ip_setsockopt(sk, level, optname, optval, optlen));

	if (optlen >= sizeof(unsigned long int)) {
		if(get_user(val, (unsigned long int *) optval))
			return (-EFAULT);
	} else if (optlen >= sizeof(char)) {
		unsigned char ucval;
		if (get_user(ucval, (unsigned char *) optval))
			return (-EFAULT);
		val = (int)ucval;
	}
	/* If optlen==0, it is equivalent to val == 0 */

	switch (optname) {
		case IPESTP_CHECKSUM:
			if (sk->sk_state == ESTP_ESTABLISHED)
				return (-EISCONN);
			for (i=1; esp_checksum_opt[i] >= 0; i++)
				EOPT_UNSET(sk, esp_checksum_opt[i]);
			if (val) {
				val = min(i - 1, val);
				EOPT_SET(sk, esp_checksum_opt[val]);
			}
			break;
#if 0
		case IPESTP_NETDEVICE:
			if (!val) {
				/* 0 = No device be bound to socket stream. */
				sk->bound_dev_if = 0;
			} else {
				if (!rt) 
					return (-ENOTCONN);
				dev = dev_get_by_index(net, (int)val);
				if (!dev)
					return (-ENODEV);
				err = ip_route_connect(&newrt, rt->rt_dst, rt->rt_src,
						sk->protinfo.af_inet.tos|sk->localroute, dev->ifindex, 0);
				if (err)
					return (err);
				if ((newrt->rt_flags&RTCF_BROADCAST) && !sk->broadcast) {
					ip_rt_put(newrt);
					return (-EACCES);
				}
				dst_release(xchg(&sk->dst_cache, &newrt->u.dst));
				sk->bound_dev_if = dev->ifindex; 
			}
			break;
#endif

#if 0
		case IPESTP_SOURCEADDR:
			if (!rt)
				return (-ENOTCONN);
			dev = ip_dev_find(val);
			if (!dev || (sk->bound_dev_if && sk->bound_dev_if != dev->ifindex))
				return (-EADDRNOTAVAIL);
			err = ip_route_connect(&newrt, rt->rt_dst, val, 
					sk->protinfo.af_inet.tos|sk->localroute, dev->ifindex);
			if (err)
				return (err);
			if ((newrt->rt_flags&RTCF_BROADCAST) && !sk->broadcast) {
				ip_rt_put(newrt);
				return (-EACCES);
			}
			dst_release(&newrt->u.dst);
			err = esp_request(sk, NULL, EREQ_SOCKET_DESTIPV4, &val);
			if (err)
				return (err);
			break;
#endif

#if 0
		case IPESTP_GATEWAY:
			if (!rt) 
				return (-ENOTCONN);
			dev = ip_dev_find(val);
			if (!dev || (sk->sk_bound_dev_if && sk->sk_bound_dev_if != dev->ifindex))
				return (-EADDRNOTAVAIL);
			err = ip_route_connect(&newrt, rt->rt_dst, rt->rt_src,
					sk->protinfo.af_inet.tos|sk->localroute, dev->ifindex, 0);
			if (err)
				return (err);
			if ((newrt->rt_flags&RTCF_BROADCAST) && !sk->broadcast) {
				ip_rt_put(newrt);
				return (-EACCES);
			}
			dst_release(xchg(&sk->sk_dst_cache, &newrt->u.dst));
			break;
#endif

		case IPESTP_LOCAL_ERROR:
			if (val >= ESTP_MAX_ERROR_CODES)
				return (-EINVAL);
			af->lcl_error = val;
			break;
		case IPESTP_REMOTE_ERROR:
			if (val >= ESTP_MAX_ERROR_CODES)
				return (-EINVAL);
			af->rem_error = val;
			break;
		case IPESTP_COMPRESS:
			if (sk->sk_state != ESTP_CLOSE)
				return (-EINVAL);
			for (i=1; esp_compress_opt[i] > 0; i++)
				EOPT_UNSET(sk, esp_compress_opt[i]);
			val = min(i - 1, val);
			if (val)
				EOPT_SET(sk, esp_compress_opt[val]);
			break;
#if 0
		case IPESTP_VERBOSE_DEBUG:
			sk->debug = !!val;
			break;
#endif
#ifndef CONFIG_ESTP_SENDWINDOW
		case IPESTP_MAXBPS:
			if (val > ESTP_MAX_DATA_WINDOW)
				return (-EINVAL);
			if (sk->sk_state == ESTP_ESTABLISHED) {
				err = esp_request(sk, NULL, EREQ_SOCKET_BPS, val);
				if (err)
					return (err);
			}
			af->locked_msw = val;
			break;
#endif
		default:
			return (-EINVAL);
	}

	return (0);
}		

static int esp_v4_getsockopt(struct sock *sk, int level, int optname, char *optval, int *optlen)
{
  struct shesp_sock *af = shesp_sk(sk);
	struct esp_remote_host *rh = af->rem_host;
	struct rtable *rt = (struct rtable *)sk->sk_dst_cache;
	unsigned long int val;
	int len;
	int i;

	if (level != SOL_ESTP)
		return (ip_getsockopt(sk, level, optname, optval, optlen));

	switch (optname) {
		case IPESTP_CHECKSUM:
			val = 0;
			for (i=1; esp_checksum_opt[i] >= 0; i++) {
				if (EOPT_ISSET(sk, esp_checksum_opt[i])) {
					val = i;
					break;
				}
			}
			break;
#ifdef CONFIG_ESTP_EXPERIMENTAL
		case IPESTP_SOURCEADDR:
			if (!rt)
				return (-ENOTCONN);
			val = rt->rt_src;
			break;
#endif
		case IPESTP_GATEWAY:
			if (!rt)
				return (-ENOTCONN);
			val = rt->rt_gateway;
			break;
		case IPESTP_NETDEVICE:
			val = sk->sk_bound_dev_if;
			break;
		case IPESTP_LOCAL_ERROR:
			val = af->lcl_error;
			break;
		case IPESTP_REMOTE_ERROR:
			val = af->rem_error; 
			break;
		case IPESTP_COMPRESS:
			val = 0;
			for (i=1; esp_compress_opt[i] >= 0; i++) {
				if (EOPT_ISSET(sk, esp_compress_opt[i])) {
					val = i;
					break;
				}
			}
			break;
#if 0
		case IPESTP_VERBOSE_DEBUG:
			val = sk->debug;
			break;
#endif
		case IPESTP_VERSION:
			val = ESTP_VERSION;
			break;
#ifdef CONFIG_ESTP_SENDWINDOW
		case IPESTP_MAXBPS:
			val = af->locked_msw;
			break;
#endif
		default:
			return (-EINVAL);
	}

	if (get_user(len, optlen))
		return (-EFAULT);

	if (len < sizeof(unsigned long int) && len > 0 && val>=0 && val<255) {
		unsigned char ucval = (unsigned char)val;
		len = 1;
		if (put_user(len, optlen))
			return (-EFAULT);
		if (copy_to_user(optval, &ucval, 1))
			return (-EFAULT);
	} else {
		len = min(sizeof(unsigned long int), len);
		if (put_user(len, optlen))
			return (-EFAULT);
		if (copy_to_user(optval, &val, len))
			return (-EFAULT);
	}

	return (0);
}

static inline struct sock *esp_v4_accept(struct sock *sk, int flags)
{
	return (esp_accept(sk, flags));
}

static inline unsigned int esp_v4_poll(struct file *file, struct socket *sock, poll_table *wait)
{
	return (esp_poll(file, sock, wait));
}

static inline void shesp_v4_hash(struct sock *sk)
{
  shesp_hash_sock(sk);
}

static inline void shesp_v4_unhash(struct sock *sk)
{
  shesp_unhash_sock(sk);
}

static inline int shesp_v4_get_port(struct sock *sk, unsigned short snum)
{
  struct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;
  struct inet_bind_hashbucket *head;
  struct hlist_node *node;
  struct inet_bind_bucket *tb;
  int ret, attempts = 5;
  struct net *net = sock_net(sk);
  int smallest_size = -1, smallest_rover;

  local_bh_disable();
  if (!snum) {
    int remaining, rover, low, high;

again:
    inet_get_local_port_range(&low, &high);
    remaining = (high - low) + 1;
    smallest_rover = rover = net_random() % remaining + low;

    smallest_size = -1;
    do {
      head = &hashinfo->bhash[inet_bhashfn(net, rover,
          hashinfo->bhash_size)];

      spin_lock(&head->lock);
      inet_bind_bucket_for_each(tb, node, &head->chain)
        if (ib_net(tb) == net && tb->port == rover) {
          if (tb->fastreuse > 0 &&
              sk->sk_reuse &&
              sk->sk_state != ESTP_LISTEN &&
              (tb->num_owners < smallest_size || smallest_size == -1)) {
            smallest_size = tb->num_owners;
            smallest_rover = rover;
            if (atomic_read(&hashinfo->bsockets) > (high - low) + 1) {
              spin_unlock(&head->lock);
              snum = smallest_rover;
              goto have_snum;
            }
          }
          goto next;
        }
      break;
next:
      spin_unlock(&head->lock);
      if (++rover > high)
        rover = low;
    } while (--remaining > 0);

    /* Exhausted local port range during search?  It is not
     * possible for us to be holding one of the bind hash
     * locks if this test triggers, because if 'remaining'
     * drops to zero, we broke out of the do/while loop at
     * the top level, not from the 'break;' statement.
     */
    ret = 1;
    if (remaining <= 0) {
      if (smallest_size != -1) {
        snum = smallest_rover;
        goto have_snum;
      }
      goto fail;
    }
    /* OK, here is the one we will use.  HEAD is
     * non-NULL and we hold it's mutex.
     */
    snum = rover;
  } else {
have_snum:
    head = &hashinfo->bhash[inet_bhashfn(net, snum,
        hashinfo->bhash_size)];
    spin_lock(&head->lock);
    inet_bind_bucket_for_each(tb, node, &head->chain)
      if (ib_net(tb) == net && tb->port == snum)
        goto tb_found;
  }
  tb = NULL;

  goto tb_not_found;
tb_found:
  if (!hlist_empty(&tb->owners)) {
    if (tb->fastreuse > 0 &&
        sk->sk_reuse && sk->sk_state != ESTP_LISTEN &&
        smallest_size == -1) {
      goto success;
    } else {
      ret = 1;
      if (inet_csk(sk)->icsk_af_ops->bind_conflict(sk, tb)) {
        if (sk->sk_reuse && sk->sk_state != ESTP_LISTEN &&
            smallest_size != -1 && --attempts >= 0) {
          spin_unlock(&head->lock);
          goto again;
        }
        goto fail_unlock;
      }
    }
  }
tb_not_found:
  ret = 1;
  if (!tb && (tb = inet_bind_bucket_create(hashinfo->bind_bucket_cachep,
          net, head, snum)) == NULL)
    goto fail_unlock;
  if (hlist_empty(&tb->owners)) {
    if (sk->sk_reuse && sk->sk_state != ESTP_LISTEN)
      tb->fastreuse = 1;
    else
      tb->fastreuse = 0;
  } else if (tb->fastreuse &&
      (!sk->sk_reuse || sk->sk_state == ESTP_LISTEN))
    tb->fastreuse = 0;
success:
  if (!inet_csk(sk)->icsk_bind_hash)
    inet_bind_hash(sk, tb, snum);
  WARN_ON(inet_csk(sk)->icsk_bind_hash != tb);
  ret = 0;

fail_unlock:
  spin_unlock(&head->lock);
fail:
  local_bh_enable();
  return ret;
}

static inline int shesp_v4_disconnect(struct sock *sk, int flags)
{
  return (shesp_disconnect(sk, flags));
}

struct proto shesp4_proto = {
	.name = "ESTP",
	.owner = THIS_MODULE,
	.close = shesp_v4_close,
	.connect = shesp_v4_connect,
	.disconnect = shesp_v4_disconnect, 
	.accept = esp_v4_accept,
	.ioctl = esp_v4_ioctl,
	.init = esp_v4_init_sock,
	.destroy = esp_v4_destroy_sock,
	.setsockopt = esp_v4_setsockopt,
	.getsockopt = esp_v4_getsockopt,
	.sendmsg = shesp_v4_sendmsg,
	.recvmsg = shesp_v4_recvmsg,
	.backlog_rcv = esp_rcv_skb,
	.max_header = sizeof(struct esphdr),
	.obj_size = sizeof(struct shesp_sock),
  .hash = shesp_v4_hash,
  .unhash = shesp_v4_unhash,
  .get_port   = shesp_v4_get_port,
  .h.hashinfo = &shesp_hashinfo,
};


static const struct net_protocol shesp4_protocol = {
  .handler = shesp_v4_rcv,
  .err_handler = shesp_v4_err,  
  .no_policy = 1,
  .netns_ok = 1,
};


static int __init shesp_init(void)
{
  int err;

  err = proto_register(&shesp4_proto, 1);
  if (err)
    return (-EAGAIN);

  if (inet_add_protocol(&shesp4_protocol, IPPROTO_ESTP) < 0) {
    printk(KERN_INFO "ip shesp init: can't add protocol\n");
    return (-EAGAIN);
  }

  printk (KERN_INFO "Registered shnet protocol (ESTP-%d).\n", ESTP_VERSION);
  return 0;
}

static void __exit shesp_term(void)
{
  if (inet_del_protocol(&shesp4_protocol, IPPROTO_ESTP) < 0) {
    printk(KERN_INFO "ip esp close: can't remove protocol\n");
  }

  proto_unregister(&shesp4_proto);
  printk (KERN_INFO "Unregistered shnet protocol (ESTP-%d).\n", ESTP_VERSION);

}

module_init(shesp_init);
module_exit(shesp_term);
MODULE_AUTHOR("Neo Natura");
MODULE_LICENSE("GPL");
MODULE_DESCRIPTION("Encoded Stream Protocol (libshare)");
